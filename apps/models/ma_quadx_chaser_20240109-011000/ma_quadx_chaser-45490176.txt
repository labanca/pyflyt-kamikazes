train_desc=' LW on and many agents since the beggining, rew_coef higher . \n                            \n                if target_id != agent_id:  #avoid the scenario where there are no targets, returns the last rewards in the last steps\n\n                # reward for closing the distance\n                self.rew_closing_distance[agent_id] = np.clip(\n                    self.previous_distance[agent_id][target_id] - self.current_distance[agent_id][target_id],\n                    a_min=-10.0,\n                    a_max=None,\n                )  \n\n                \n\n                exploding_distance = self.current_distance[agent_id][target_id] - 0.5\n\n                self.rew_close_to_target[agent_id] = - exploding_distance\n                 \n                # self.rew_close_to_target[agent_id] = 1 / (exploding_distance\n                #                                 if exploding_distance > 0\n                #                                 else 0.09)   #if the 1 is to hight the kamikazes will circle the enemy. try a\n\n\n            self.rewards[agent_id] += (\n                    self.rew_closing_distance[agent_id]\n                    + self.rew_close_to_target[agent_id] * self.reward_coef #* (1 - self.step_count/self.max_steps) # regularizations\n\n            )\n            \n            \n\n'
__file__='C:\\projects\\pyflyt-kamikazes\\apps\\resume_train.py'
model.start_datetime=2024-01-09 09:02:26.349849
elapsed_time=0:14:24.324022
model_name=ma_quadx_chaser-44212224.zip
model_name=ma_quadx_chaser-45490176
model.num_timesteps=45744128
lw_stand_still=False
device=device(type='cuda')
seed=0
batch_size=1024
model.learning_rate=0.0001
discount_factor=0.99
nn_t=[256, 256, 256]
num_cpus=16
num_vec_envs=16
model.n_envs=208
model.n_steps=2048
model.n_epochs=10
model.batch_size=1024
random_respawn=True
spawn_settings={'lw_center_bounds': 10.0, 'lw_spawn_radius': 2.0, 'lm_center_bounds': 5.0, 'lm_spawn_radius': 10.0, 'min_z': 0.5, 'seed': None, 'num_lw': 4, 'num_lm': 13}
completion_datetime=2024-01-09 09:16:50.673871
model.action_space=Box(-10.0, 10.0, (4,), float64)
model.observation_space=Box(-inf, inf, (40,), float64)
env_kwargs={'start_pos': array([[ -3.67743594, -11.11700414,  10.58418145],
       [ -3.88768153, -11.52767528,   6.65095803],
       [ -2.07409033, -12.75990118,   8.16356163],
       [  8.83492427, -12.17542736,   4.47547615],
       [ -2.80378333, -12.48847695,   7.16125957],
       [ -6.23160827, -11.10362035,   1.83172993],
       [ -2.92956013, -12.7325911 ,   1.02926605],
       [  0.69259989, -13.06445222,  10.52763478],
       [ 12.85481316, -12.54923251,  10.38657634],
       [ -3.89789559, -10.89215654,   4.67850592],
       [ -0.21227746, -12.26962098,   8.90985259],
       [ -5.63884037, -12.90518249,   4.90213328],
       [  7.92898828, -12.50973254,   8.56382262],
       [  5.77706188,  -0.93749569,   7.42872551],
       [  3.77706188,   1.06250431,   7.42872551],
       [  1.77706188,  -0.93749569,   7.42872551],
       [  3.77706188,  -2.93749569,   7.42872551]]), 'start_orn': array([[ 0.18703167,  0.96973022,  4.84242672],
       [ 0.03712828, -0.94122793, -5.00660146],
       [-0.78451036, -0.06610692,  0.13293285],
       [ 0.71783118,  0.9039976 ,  3.24379764],
       [ 0.45780467, -0.17431625,  3.75186826],
       [ 0.09466234,  0.48346513,  2.13238355],
       [ 0.75178869, -0.79871632, -3.15718996],
       [ 0.31984907, -0.8685183 , -0.59936609],
       [ 0.58196424,  0.15746004,  0.5102822 ],
       [-0.92612435,  0.54838281, -5.12907561],
       [-0.48537687,  0.0400809 , -6.1910044 ],
       [ 0.39525478,  0.65878768, -3.77185679],
       [-0.33360548,  0.68062886, -4.91170718],
       [ 0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ],
       [ 0.        ,  0.        ,  0.        ]]), 'formation_center': [3.7770618775158393, -0.9374956883514756, 7.428725511036053], 'flight_dome_size': 40.5, 'seed': None, 'spawn_settings': {'lw_center_bounds': 10.0, 'lw_spawn_radius': 2.0, 'lm_center_bounds': 5.0, 'lm_spawn_radius': 10.0, 'min_z': 0.5, 'seed': None, 'num_lw': 4, 'num_lm': 13}, 'num_lm': 13, 'num_lw': 4, 'max_duration_seconds': 15, 'distance_factor': 0.1, 'speed_factor': 16.0, 'lw_stand_still': False, 'rew_exploding_target': 200}
train_kwargs={'device': device(type='cuda'), 'batch_size': 1024, 'lr': 0.0001, 'discount_factor': 0.99, 'nn_t': [256, 256, 256], 'num_vec_envs': 16}
model.policy_kwargs={'net_arch': {'pi': [256, 256, 256], 'vf': [256, 256, 256]}}
model.policy=ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=40, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): Tanh()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=40, out_features=256, bias=True)
      (1): Tanh()
      (2): Linear(in_features=256, out_features=256, bias=True)
      (3): Tanh()
      (4): Linear(in_features=256, out_features=256, bias=True)
      (5): Tanh()
    )
  )
  (action_net): Linear(in_features=256, out_features=4, bias=True)
  (value_net): Linear(in_features=256, out_features=1, bias=True)
)
model.policy_aliases={'MlpPolicy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'CnnPolicy': <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>}
model.policy_class=<class 'stable_baselines3.common.policies.ActorCriticPolicy'>
