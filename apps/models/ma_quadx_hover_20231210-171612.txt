random_respawn=True
__file__='C:\\projects\\pyflyt_parallel\\apps\\hover\\train.py'
model.num_timesteps=10158080
device=device(type='cpu')
seed=0
model.start_datetime=2023-12-10 16:02:15.399804
completion_datetime=2023-12-10 17:16:12.912263
elapsed_time=1:13:57.512459
num_cpus=8
num_vec_envs=8
model.device=device(type='cpu')
model.learning_rate=7e-05
env_kwargs={'start_pos': array([[-9.86536525, -8.30625229,  1.84427054],
       [ 0.94359197,  3.22007443,  5.29228799],
       [ 2.49366383,  4.26377348,  7.40507826],
       [-1.40113477,  1.88316542,  8.63043978],
       [ 3.10834009, -3.20161498,  5.93361771],
       [ 0.01080202, -8.75195555,  2.02680853],
       [-6.71010012, -9.9205055 ,  6.15379715],
       [-2.16734405, -1.98010944,  1.60907062],
       [-6.37308497,  5.10027752,  1.96747078],
       [-4.99704   ,  6.27209529,  6.0243676 ],
       [-4.91885808,  5.04269199,  9.06019686],
       [ 7.1982346 ,  1.02635428,  2.43747575],
       [-2.72826063, -6.25565014,  3.45606348],
       [-5.33258936,  0.12946721,  9.97083226],
       [ 5.46603375,  8.56998753,  7.46562548],
       [-4.57504526,  1.2371099 ,  1.73028307],
       [-5.74729548,  3.82444952,  7.44433109],
       [ 9.19245307,  9.83788484,  6.1945346 ],
       [ 8.49594309, -1.07984479,  8.04401636],
       [ 4.89680961,  0.23695819,  5.93766322]]), 'start_orn': array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]]), 'flight_dome_size': 28.578838324886476, 'spawn_settings': {'num_drones': 20, 'min_distance': 2.0, 'spawn_radius': 10.0, 'center': (0, 0, 0)}}
spawn_settings={'num_drones': 20, 'min_distance': 2.0, 'spawn_radius': 10.0, 'center': (0, 0, 0)}
model.policy_kwargs={'net_arch': {'pi': [128, 128, 128], 'vf': [128, 128, 128]}}
model.policy=ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=24, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=24, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
    )
  )
  (action_net): Linear(in_features=128, out_features=4, bias=True)
  (value_net): Linear(in_features=128, out_features=1, bias=True)
)
model.policy_aliases={'MlpPolicy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'CnnPolicy': <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>}
model.policy_class=<class 'stable_baselines3.common.policies.ActorCriticPolicy'>
model.n_envs=160
