results: its seems the best one. No fallings with time, instantly hover.
random_respawn=True
__file__='C:\\projects\\pyflyt_parallel\\train.py'
model.num_timesteps=40304640
device=device(type='cpu')
seed=0
model.start_datetime=2023-12-09 01:25:03.311911
completion_datetime=2023-12-09 07:47:21.243238
elapsed_time=6:22:17.931327
num_cpus=8
num_vec_envs=8
model.device=device(type='cpu')
model.learning_rate=7e-05
env_kwargs={'start_pos': array([[ 6.71239859, -0.81921957,  4.2888622 ],
       [ 9.26683506,  6.14923426,  3.71740144],
       [ 5.31055493,  1.05966568,  5.55271983],
       [ 3.54100136,  8.75001069,  4.08433686],
       [ 3.60858075, -9.01365855,  1.83286047],
       [-8.67943339, -6.89914928,  8.14901959],
       [-5.12772459, -4.38555311,  8.52532156],
       [ 3.48598102, -6.14681653,  6.42189006],
       [ 7.89187742, -5.89331408,  8.51830612],
       [-7.47094548, -4.6365617 ,  4.69395653],
       [-7.81300253,  3.11398303,  5.72556915],
       [ 7.32406566,  9.95196807,  5.83959841],
       [ 9.50693619,  4.66025637,  6.73828661],
       [-1.97825891, -0.21079602,  4.50041392],
       [ 9.76708046,  9.70897334,  2.46212777],
       [-8.71378477, -3.40359537,  8.0553498 ],
       [-1.82240401, -1.31346626,  7.59051014],
       [-4.32727538, -3.16788831,  2.18691231],
       [ 3.13628455, -6.74663299,  1.27774186],
       [-3.75152923,  6.1144932 ,  4.30395595]]), 'start_orn': array([[0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.],
       [0., 0., 0.]]), 'flight_dome_size': 28.578838324886476, 'spawn_settings': {'num_drones': 20, 'min_distance': 2.0, 'spawn_radius': 10.0, 'center': (0, 0, 0)}}
spawn_settings={'num_drones': 20, 'min_distance': 2.0, 'spawn_radius': 10.0, 'center': (0, 0, 0)}
model.policy_kwargs={'net_arch': {'pi': [128, 128, 128], 'vf': [128, 128, 128]}}
model.policy=ActorCriticPolicy(
  (features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (pi_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (vf_features_extractor): FlattenExtractor(
    (flatten): Flatten(start_dim=1, end_dim=-1)
  )
  (mlp_extractor): MlpExtractor(
    (policy_net): Sequential(
      (0): Linear(in_features=24, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
    )
    (value_net): Sequential(
      (0): Linear(in_features=24, out_features=128, bias=True)
      (1): Tanh()
      (2): Linear(in_features=128, out_features=128, bias=True)
      (3): Tanh()
      (4): Linear(in_features=128, out_features=128, bias=True)
      (5): Tanh()
    )
  )
  (action_net): Linear(in_features=128, out_features=4, bias=True)
  (value_net): Linear(in_features=128, out_features=1, bias=True)
)
model.policy_aliases={'MlpPolicy': <class 'stable_baselines3.common.policies.ActorCriticPolicy'>, 'CnnPolicy': <class 'stable_baselines3.common.policies.ActorCriticCnnPolicy'>, 'MultiInputPolicy': <class 'stable_baselines3.common.policies.MultiInputActorCriticPolicy'>}
model.policy_class=<class 'stable_baselines3.common.policies.ActorCriticPolicy'>
model.n_envs=160
reward=def compute_term_trunc_reward_info_by_id(
        self, agent_id: int
    ) -> tuple[bool, bool, float, dict[str, Any]]:
        """Computes the termination, truncation, and reward of the current timestep."""
        term, trunc, reward, info = super().compute_base_term_trunc_reward_info_by_id(
            agent_id
        )

        if not self.sparse_reward:
            # distance from 0, 0, 1 hover point
            linear_distance = np.linalg.norm(
                self.aviary.state(agent_id)[-1] - self.start_pos[agent_id]
            )

            # how far are we from 0 roll pitch
            angular_distance = np.linalg.norm(self.aviary.state(agent_id)[1][:2])

            reward -= float(linear_distance + angular_distance * 0.1)
            reward += 0.5

        return term, trunc, reward, info

state=np.array(
                [
                    *ang_vel,
                    *quaternion,
                    *lin_vel,
                    *lin_pos,
                    *aux_state,
                    *self.past_actions[agent_id],
                    *self.start_pos[agent_id] - lin_pos,
                ]
            )